---
title: "WiDS Datathon++ 2025 - Modeling and Prediction"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
set.seed(42)
```

# 1. 📦 加载必要包

```{r}
library(tidyverse)
library(forcats)
library(caret)
library(glmnet)
library(Metrics)
library(ggplot2)
library(xgboost)
library(broom)
library(doParallel)
library(ParBayesianOptimization)
```

# 2. 🧹 数据读取与预处理



# 3. 🔍 PCA降维（保留90%信息）



# 4. 🧠 元数据处理 + 编码


# 5. 🔗 合并PCA和元数据

```{r}
train_pc_df <- train_pc %>% mutate(participant_id = train$participant_id)
valid_pc_df <- valid_pc %>% mutate(participant_id = valid$participant_id)

train_merged <- inner_join(train_pc_df, train_meta_enc, by = "participant_id")
valid_merged <- inner_join(valid_pc_df, valid_meta_enc, by = "participant_id")

X_train <- train_merged %>% select(-participant_id)
y_train <- train %>% filter(participant_id %in% train_merged$participant_id) %>% pull(age)
X_valid <- valid_merged %>% select(-participant_id)
y_valid <- valid %>% filter(participant_id %in% valid_merged$participant_id) %>% pull(age)
```

# 6. 🔎 Lasso特征选择（只针对Metadata）

```{r}
X_train_meta <- train_meta_enc %>% select(-participant_id)
X_meta_mat <- as.matrix(X_train_meta)

cv_lasso <- cv.glmnet(X_meta_mat, y_train, alpha = 1, nfolds = 10, standardize = TRUE, type.measure = "mse")
lasso_coefs <- coef(cv_lasso, s = "lambda.min")
lasso_selected <- rownames(lasso_coefs)[lasso_coefs[, 1] != 0 & rownames(lasso_coefs) != "(Intercept)"]

cat("✅ 选中的重要metadata特征：\n")
print(lasso_selected)
```

# 7. 📈 Ridge回归

## 7.1 Ridge (全PCA+Metadata)

```{r}
X_train_mat <- as.matrix(X_train)
X_valid_mat <- as.matrix(X_valid)
# 手动创建固定的foldid

ridge_pred <- predict(cv_ridge, newx = X_valid_mat, s = "lambda.min") %>% as.vector()
ridge_rmse <- rmse(y_valid, ridge_pred)

cat("✅ Ridge 回归验证集 RMSE:", round(ridge_rmse, 4), "\n")

# 可视化
ridge_plot <- ggplot(data.frame(Predicted = ridge_pred, Actual = y_valid), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Ridge Regression: Predicted vs Actual Age") +
  theme_minimal()
print(ridge_plot)

```

## 7.2 Ridge (PCA + Lasso筛选Metadata)

```{r}

ridge_lasso_pca_pred <- predict(cv_ridge_lasso_pca, newx = X_valid_lasso_pca_mat, s = "lambda.min") %>% as.vector()
ridge_lasso_pca_rmse <- rmse(y_valid, ridge_lasso_pca_pred)

cat("✅ Ridge (Lasso Metadata + PCA) 验证集 RMSE:", round(ridge_lasso_pca_rmse, 4), "\n")
```


```{r}
library(glmnet)
library(caret)
library(Metrics)


}

# ✅ 计算平均预测 + 标准差
ridge_matrix <- do.call(cbind, ridge_preds)
ridge_lasso_pca_pred <- rowMeans(ridge_matrix)
ridge_lasso_pca_sd <- apply(ridge_matrix, 1, sd)  # 可用于残差稳定性分析

# ✅ 计算 RMSE
ridge_lasso_pca_rmse <- rmse(y_valid, ridge_lasso_pca_pred)

cat("✅ Ridge (5-Fold Averaging) 验证集 RMSE:", round(ridge_lasso_pca_rmse, 4), "\n")

# ✅ 可视化
ggplot(data.frame(Predicted = ridge_lasso_pca_pred, Actual = y_valid), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Ridge (5-Fold Ensemble): Predicted vs Actual Age") +
  theme_minimal()

```



  bounds = list(
    eta = c(0.03, 0.1),
    max_depth = c(3L, 6L)
  ),
  initPoints = 5, iters.n = 10, acq = "ucb", verbose = 1
)

best_params <- getBestPars(opt)



xgb_pred <- predict(xgb_final, newdata = dvalid)
xgb_rmse <- rmse(y_valid, xgb_pred)

cat("✅ Final Tuned XGBoost 验证集 RMSE:", round(xgb_rmse, 4), "\n")

# 可视化
xgb_plot <- ggplot(data.frame(Predicted = xgb_pred, Actual = y_valid), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "#1f77b4") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "darkgray") +
  labs(title = "Tuned XGBoost (Bayesian Optimization)", x = "Actual Age", y = "Predicted Age") +
  theme_minimal()
print(xgb_plot)
```

# 9. 🔁 模型融合：Ridge + XGBoost

```{r}
rame(Weight = weights, RMSE = ensemble_rmse), aes(x = Weight, y = RMSE)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = best_w, linetype = "dashed", color = "red") +
  labs(title = "Ensemble Weight vs Validation RMSE", x = "Ridge Weight", y = "Validation RMSE") +
  theme_minimal()
print(ensemble_plot)
```




```{r}
# Ridge 残差图
resid_ridge <- y_valid - ridge_lasso_pca_pred
ggplot(data.frame(Residual = resid_ridge), aes(x = Residual)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Ridge Residual Distribution", x = "Residual (y - y_hat)", y = "Count")

# XGBoost 残差图
resid_xgb <- y_valid - xgb_pred
ggplot(data.frame(Residual = resid_xgb), aes(x = Residual)) +
  geom_histogram(bins = 30, fill = "darkorange", alpha = 0.7) +
  theme_minimal() +
  labs(title = "XGBoost Residual Distribution", x = "Residual (y - y_hat)", y = "Count")

# 融合模型残差图
resid_ensemble <- y_valid - (best_w * ridge_lasso_pca_pred + (1 - best_w) * xgb_pred)
ggplot(data.frame(Residual = resid_ensemble), aes(x = Residual)) +
  geom_histogram(bins = 30, fill = "darkgreen", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Ensemble Residual Distribution", x = "Residual", y = "Count")

```

```{r}
library(e1071)

# 假设 residuals 都是向量
ridge_resid <- ridge_pred - y_valid
xgb_resid   <- xgb_pred   - y_valid
ensemble_resid <- best_w * ridge_pred + (1 - best_w) * xgb_pred - y_valid

resid_stats <- tibble(
  Model     = c("Ridge", "XGBoost", "Ensemble"),
  Skewness  = c(skewness(ridge_resid), skewness(xgb_resid), skewness(ensemble_resid)),
  Kurtosis  = c(kurtosis(ridge_resid), kurtosis(xgb_resid), kurtosis(ensemble_resid))
)

print(resid_stats)

```


```{r}
library(glmnet)

# 1. 构建二级训练数据
stack_train <- data.frame(
  Ridge = ridge_lasso_pca_pred,
  XGB = xgb_pred
)

dstack <- xgb.DMatrix(as.matrix(stack_train), label = y_valid)


```


------------------------------------------------------------------------
