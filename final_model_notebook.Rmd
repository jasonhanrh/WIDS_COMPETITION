
---
title: "R Notebook"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 📦 加载必要包
```{r}
library(tidyverse)
library(forcats)
library(caret)
library(glmnet)
library(Metrics)
library(ggplot2)
library(xgboost)
library(broom)
library(doParallel)
library(ParBayesianOptimization)
```

# 🧹 数据读取与预处理
```{r}
df <- readRDS("train_df.rds")
set.seed(42)
train_idx <- createDataPartition(df$sex, p = 0.8, list = FALSE)
train <- df[train_idx, ]
valid <- df[-train_idx, ]
```

# 🔍 PCA 降维（保留90%信息）
```{r}
pcs <- prcomp(select(train, starts_with("V")), center = TRUE, scale. = TRUE)
explained_var <- summary(pcs)$importance[2, ]
cum_explained <- cumsum(explained_var)
n_pcs_90 <- which(cum_explained >= 0.90)[1]
cat("✅ 建议保留的主成分数量:", n_pcs_90, "\n")
train_pc <- as_tibble(pcs$x[, 1:n_pcs_90])
valid_pc <- predict(pcs, select(valid, starts_with("V")))[, 1:n_pcs_90] %>% as_tibble()
```

# 🧠 元数据处理 + 编码
```{r}
prepare_meta <- function(df) {
  df %>%
    select(participant_id, sex, bmi, p_factor_fs, internalizing_fs, externalizing_fs, attention_fs,
           race, study_site, handedness, parent_1_education) %>%
    mutate(
      bmi = ifelse(is.na(bmi), median(bmi, na.rm = TRUE), bmi),
      across(c(sex, race, study_site, handedness, parent_1_education), ~ fct_na_value_to_level(factor(.), "Missing"))
    )
}
encode <- function(data) model.matrix(~ . -1, data = data)
train_meta <- prepare_meta(train)
valid_meta <- prepare_meta(valid)
train_meta_enc <- bind_cols(as_tibble(encode(train_meta %>% select(-participant_id))), tibble(participant_id = train_meta$participant_id))
valid_meta_enc <- bind_cols(as_tibble(encode(valid_meta %>% select(-participant_id))), tibble(participant_id = valid_meta$participant_id))
```

# 🔗 合并 PCA 和元数据
```{r}
train_pc_df <- train_pc %>% mutate(participant_id = train$participant_id)
valid_pc_df <- valid_pc %>% mutate(participant_id = valid$participant_id)
train_merged <- inner_join(train_pc_df, train_meta_enc, by = "participant_id")
valid_merged <- inner_join(valid_pc_df, valid_meta_enc, by = "participant_id")
X_train <- train_merged %>% select(-participant_id)
y_train <- train %>% filter(participant_id %in% train_merged$participant_id) %>% pull(age)
X_valid <- valid_merged %>% select(-participant_id)
y_valid <- valid %>% filter(participant_id %in% valid_merged$participant_id) %>% pull(age)
```

# 🔎 Lasso 特征选择
```{r}
X_train_meta <- train_meta_enc %>% select(-participant_id)
X_meta_mat <- as.matrix(X_train_meta)
cv_lasso <- cv.glmnet(X_meta_mat, y_train, alpha = 1, nfolds = 10, standardize = TRUE)
lasso_coefs <- coef(cv_lasso, s = "lambda.min")
lasso_selected <- rownames(lasso_coefs)[lasso_coefs[, 1] != 0 & rownames(lasso_coefs) != "(Intercept)"]
cat("✅ 选中的重要 metadata 特征：\n")
print(lasso_selected)
```
# 📈 Ridge Regression（Full Metadata + PCA）
```{r}
X_train_mat <- as.matrix(X_train)
X_valid_mat <- as.matrix(X_valid)

set.seed(42)
cv_ridge <- cv.glmnet(X_train_mat, y_train, alpha = 0, nfolds = 10, standardize = TRUE)

ridge_pred <- predict(cv_ridge, newx = X_valid_mat, s = "lambda.min") %>% as.vector()
ridge_rmse <- rmse(y_valid, ridge_pred)
cat("✅ Ridge 回归验证集 RMSE:", round(ridge_rmse, 4), "\n")

ggplot(data.frame(Predicted = ridge_pred, Actual = y_valid), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Ridge Regression: Predicted vs Actual Age") +
  theme_minimal()
```





# 📈 Ridge Regression（Lasso Metadata + PCA）
```{r}
X_train_lasso_meta <- X_train_meta[, lasso_selected]
X_valid_lasso_meta <- X_valid %>% select(all_of(colnames(X_train_lasso_meta)))
X_train_lasso_pca <- cbind(train_pc, X_train_lasso_meta)
X_valid_lasso_pca <- cbind(valid_pc, X_valid_lasso_meta)
X_train_lasso_pca_mat <- as.matrix(X_train_lasso_pca)
X_valid_lasso_pca_mat <- as.matrix(X_valid_lasso_pca)
cv_ridge_lasso_pca <- cv.glmnet(X_train_lasso_pca_mat, y_train, alpha = 0, nfolds = 10)
ridge_lasso_pca_pred <- predict(cv_ridge_lasso_pca, newx = X_valid_lasso_pca_mat, s = "lambda.min") %>% as.vector()
ridge_lasso_pca_rmse <- rmse(y_valid, ridge_lasso_pca_pred)
cat("✅ Ridge (Lasso Metadata + PCA) 验证集 RMSE:", round(ridge_lasso_pca_rmse, 4), "\n")
```

# 🤖 XGBoost + Tuned (Bayesian Optimization)
```{r}
# 📦 必要包
library(xgboost)
library(ParBayesianOptimization)
library(Metrics)

# ✅ 1. 准备数据
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dvalid <- xgb.DMatrix(data = as.matrix(X_valid), label = y_valid)

# ✅ 2. 定义优化目标函数
bayes_xgb_function <- function(eta, max_depth) {
  params <- list(
    booster = "gbtree",
    objective = "reg:squarederror",
    eval_metric = "rmse",
    tree_method = "hist",
    eta = eta,
    max_depth = as.integer(max_depth),
    subsample = 0.8,           # 固定，加快速度
    colsample_bytree = 0.8      # 固定，加快速度
  )
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,              # 训练步数减少，加速
    watchlist = list(valid = dvalid),
    early_stopping_rounds = 10,
    verbose = 0
  )
  list(Score = -rmse(y_valid, predict(model, dvalid)))
}

# ✅ 3. 设置搜索空间
opt <- bayesOpt(
  FUN = bayes_xgb_function,
  bounds = list(
    eta = c(0.03, 0.1),        # 收窄 eta
    max_depth = c(3L, 6L)      # 收窄 max_depth
  ),
  initPoints = 5,
  iters.n = 10,                # 总共只跑10次，大幅加速
  acq = "ucb",
  verbose = 1
)

# ✅ 4. 取得最优参数
best_params <- getBestPars(opt)

# ✅ 5. 用最优参数重新训练
xgb_final <- xgb.train(
  params = list(
    booster = "gbtree", objective = "reg:squarederror", eval_metric = "rmse",
    eta = best_params$eta, max_depth = as.integer(best_params$max_depth),
    subsample = 0.8, colsample_bytree = 0.8, tree_method = "hist"
  ),
  data = dtrain,
  nrounds = 200,              # 正式训练多一点
  watchlist = list(valid = dvalid),
  early_stopping_rounds = 10,
  verbose = 1
)

# ✅ 6. 验证集预测
xgb_pred <- predict(xgb_final, newdata = dvalid)
xgb_rmse <- rmse(y_valid, xgb_pred)
cat("✅ Final Tuned XGBoost 验证集 RMSE:", round(xgb_rmse, 4), "\n")

# ✅ 7. 可视化预测效果
library(ggplot2)
ggplot(data.frame(Predicted = xgb_pred, Actual = y_valid), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "#1f77b4") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "darkgray") +
  labs(
    title = "Tuned XGBoost (Bayesian Optimization)",
    x = "Actual Age", y = "Predicted Age"
  ) +
  theme_minimal()


```

# 🔁 模型融合：Ridge + XGBoost
```{r}
weights <- seq(0.1, 0.9, by = 0.1)
ensemble_rmse <- sapply(weights, function(w) {
  rmse(y_valid, w * ridge_lasso_pca_pred + (1 - w) * xgb_pred)
})
best_w <- weights[which.min(ensemble_rmse)]
cat("✅ 最佳融合权重 w =", best_w, "对应 RMSE =", round(min(ensemble_rmse), 4), "\n")
```




